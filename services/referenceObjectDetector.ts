// services/referenceObjectDetector.ts
// D√©tecteur sp√©cialis√© d'objets de r√©f√©rence pour la calibration d'image

import OpenAI from 'openai';
import { logger as loggingService } from './core/loggingService';

export interface DetectedReference {
  type: 'door' | 'doorFrame' | 'outlet' | 'lightSwitch' | 'tile' | 'baseboard' | 'window';
  label: string;
  confidence: number;
  pixelDimensions: {
    width: number;
    height: number;
    x?: number;
    y?: number;
  };
  standardDimension: {
    width?: number;  // en cm
    height?: number; // en cm
  };
  notes?: string;
  quality: 'excellent' | 'good' | 'fair' | 'poor';
}

export interface ReferenceDetectionResult {
  references: DetectedReference[];
  imageInfo: {
    width?: number;
    height?: number;
    quality: string;
  };
  detectionTime: number;
  totalReferences: number;
}

export class ReferenceObjectDetector {
  private openaiClient: OpenAI;

  constructor() {
    this.openaiClient = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    });
  }

  /**
   * D√©tecte tous les objets de r√©f√©rence dans une image
   */
  async detectReferences(imageUrl: string): Promise<ReferenceDetectionResult> {
    const startTime = Date.now();

    try {
      loggingService.info('D√©tection d\'objets de r√©f√©rence...', 'ReferenceObjectDetector');

      const prompt = this.buildDetectionPrompt();
      const response = await this.openaiClient.chat.completions.create({
        model: "gpt-4o",
        temperature: 0.1, // Tr√®s pr√©cis
        max_tokens: 2000,
        response_format: { type: "json_object" },
        messages: [
          {
            role: "user",
            content: [
              { type: "text", text: prompt },
              {
                type: "image_url",
                image_url: {
                  url: imageUrl,
                  detail: "high" // Haute r√©solution pour pr√©cision
                }
              }
            ]
          }
        ]
      });

      const result = JSON.parse(response.choices[0]?.message?.content || "{}");
      
      // Convertir les r√©sultats en format DetectedReference
      const references = this.parseDetectionResults(result);
      
      // √âvaluer la qualit√© de chaque r√©f√©rence
      const qualityReferences = references.map(ref => this.evaluateReferenceQuality(ref));

      const detectionTime = Date.now() - startTime;

      loggingService.info(
        `${qualityReferences.length} r√©f√©rence(s) d√©tect√©e(s) en ${detectionTime}ms`,
        'ReferenceObjectDetector'
      );

      return {
        references: qualityReferences,
        imageInfo: result.imageInfo || { quality: 'unknown' },
        detectionTime,
        totalReferences: qualityReferences.length
      };

    } catch (error) {
      loggingService.error('Erreur d√©tection r√©f√©rences', 'ReferenceObjectDetector', error);
      return {
        references: [],
        imageInfo: { quality: 'error' },
        detectionTime: Date.now() - startTime,
        totalReferences: 0
      };
    }
  }

  /**
   * Construit le prompt de d√©tection optimis√©
   */
  private buildDetectionPrompt(): string {
    return `
Tu es un expert en vision par ordinateur sp√©cialis√© dans la d√©tection d'objets de r√©f√©rence pour la calibration d'images.

MISSION : D√©tecter TOUS les objets de r√©f√©rence dans cette image avec leurs dimensions EXACTES en PIXELS.

OBJETS DE R√âF√âRENCE √Ä D√âTECTER (par ordre de priorit√©) :

üö™ 1. PORTES & CADRES DE PORTE
- Porte standard : 80-90cm de large, 200-210cm de haut
- CRITIQUE : Mesure la largeur ET hauteur en pixels
- Note si porte ouverte/ferm√©e/partiellement visible

üîå 2. PRISES √âLECTRIQUES
- Dimensions : 8-10cm de large, 8-10cm de haut
- Position typique : 15-20cm du sol
- IMPORTANT : Donne coordonn√©es X,Y et dimensions

üí° 3. INTERRUPTEURS
- Dimensions : 8-10cm de large, 10-15cm de haut
- Position typique : 110-130cm du sol
- Note le nombre de boutons (simple/double)

üü¶ 4. CARRELAGE
- Tailles courantes : 30x30cm, 40x40cm, 60x60cm
- IMPORTANT : Compte plusieurs carreaux pour pr√©cision
- Note la couleur et le motif de joints

ü™ü 5. FEN√äTRES
- Largeur typique : 60-150cm
- Hauteur typique : 100-150cm
- Note si visible enti√®rement

üìè 6. PLINTHES
- Hauteur typique : 8-15cm
- Visible le long des murs
- Mesure la hauteur en pixels

INSTRUCTIONS CRITIQUES :

1. PR√âCISION DES MESURES
   - Utilise ton analyse visuelle pour mesurer en pixels
   - Arrondis au pixel pr√®s
   - Si plusieurs instances, liste-les toutes

2. CONFIANCE
   - Donne une confiance 0-1 pour chaque d√©tection
   - 0.9+ : Objet parfaitement visible et mesurable
   - 0.7-0.9 : Objet clair mais partiellement occult√©
   - 0.5-0.7 : Objet visible mais mesure incertaine
   - <0.5 : Objet possible mais peu s√ªr

3. CONTEXTE
   - Note la position relative (haut/bas/gauche/droite)
   - Indique si l'objet est partiellement visible
   - Mentionne les occlusions √©ventuelles

4. QUALIT√â IMAGE
   - R√©solution : haute/moyenne/basse
   - √âclairage : bon/moyen/faible
   - Nettet√© : nette/floue

R√âPONSE AU FORMAT JSON STRICT :

{
  "references": [
    {
      "type": "door" | "doorFrame" | "outlet" | "lightSwitch" | "tile" | "baseboard" | "window",
      "label": "Description d√©taill√©e",
      "confidence": 0.95,
      "pixelWidth": 120,
      "pixelHeight": 250,
      "positionX": 500,
      "positionY": 100,
      "standardWidth": 80,
      "standardHeight": 200,
      "notes": "Porte blanche, pleinement visible, ferm√©e",
      "partiallyVisible": false,
      "occluded": false
    }
  ],
  "imageInfo": {
    "estimatedWidth": 1920,
    "estimatedHeight": 1080,
    "resolution": "high",
    "lighting": "good",
    "sharpness": "sharp"
  }
}

IMPORTANT : D√©tecte TOUS les objets de r√©f√©rence visibles, m√™me partiellement !
`;
  }

  /**
   * Parse les r√©sultats de d√©tection
   */
  private parseDetectionResults(result: any): DetectedReference[] {
    if (!result.references || !Array.isArray(result.references)) {
      return [];
    }

    return result.references.map((ref: any) => {
      const standardDim = this.getStandardDimensions(ref.type, ref);

      return {
        type: ref.type,
        label: ref.label || ref.type,
        confidence: ref.confidence || 0.5,
        pixelDimensions: {
          width: ref.pixelWidth || 0,
          height: ref.pixelHeight || 0,
          x: ref.positionX,
          y: ref.positionY
        },
        standardDimension: standardDim,
        notes: this.buildNotes(ref),
        quality: 'good' // Sera √©valu√© apr√®s
      };
    }).filter((ref: DetectedReference) => 
      ref.pixelDimensions.width > 0 || ref.pixelDimensions.height > 0
    );
  }

  /**
   * Obtient les dimensions standard pour un type de r√©f√©rence
   */
  private getStandardDimensions(type: string, ref: any): { width?: number; height?: number } {
    // Utiliser les dimensions fournies par GPT-4 si disponibles
    if (ref.standardWidth || ref.standardHeight) {
      return {
        width: ref.standardWidth,
        height: ref.standardHeight
      };
    }

    // Sinon, utiliser les valeurs par d√©faut
    const standards: Record<string, { width?: number; height?: number }> = {
      door: { width: 80, height: 200 },
      doorFrame: { width: 90, height: 210 },
      outlet: { width: 8, height: 8 },
      lightSwitch: { width: 8, height: 12 },
      tile: { width: 30, height: 30 }, // Valeur par d√©faut, peut √™tre 40 ou 60
      baseboard: { height: 10 },
      window: { width: 100, height: 120 }
    };

    return standards[type] || {};
  }

  /**
   * Construit les notes pour une r√©f√©rence
   */
  private buildNotes(ref: any): string {
    const notes: string[] = [];

    if (ref.notes) {
      notes.push(ref.notes);
    }

    if (ref.partiallyVisible) {
      notes.push('Partiellement visible');
    }

    if (ref.occluded) {
      notes.push('Partiellement occult√©');
    }

    if (ref.lighting) {
      notes.push(`√âclairage: ${ref.lighting}`);
    }

    return notes.join('. ');
  }

  /**
   * √âvalue la qualit√© d'une r√©f√©rence d√©tect√©e
   */
  private evaluateReferenceQuality(ref: DetectedReference): DetectedReference {
    let quality: 'excellent' | 'good' | 'fair' | 'poor' = 'good';

    // Crit√®res de qualit√©
    const hasGoodConfidence = ref.confidence >= 0.85;
    const hasBothDimensions = ref.pixelDimensions.width > 0 && ref.pixelDimensions.height > 0;
    const hasPosition = ref.pixelDimensions.x !== undefined && ref.pixelDimensions.y !== undefined;
    const hasReasonableSize = 
      ref.pixelDimensions.width >= 10 && 
      ref.pixelDimensions.height >= 10 &&
      ref.pixelDimensions.width <= 2000 &&
      ref.pixelDimensions.height <= 2000;

    // √âvaluation
    if (hasGoodConfidence && hasBothDimensions && hasPosition && hasReasonableSize) {
      quality = 'excellent';
    } else if (hasGoodConfidence && hasBothDimensions && hasReasonableSize) {
      quality = 'good';
    } else if (ref.confidence >= 0.6 && (ref.pixelDimensions.width > 0 || ref.pixelDimensions.height > 0)) {
      quality = 'fair';
    } else {
      quality = 'poor';
    }

    return {
      ...ref,
      quality
    };
  }

  /**
   * Filtre les r√©f√©rences par qualit√© minimale
   */
  filterByQuality(
    references: DetectedReference[],
    minQuality: 'excellent' | 'good' | 'fair' | 'poor' = 'fair'
  ): DetectedReference[] {
    const qualityOrder = { excellent: 4, good: 3, fair: 2, poor: 1 };
    const minLevel = qualityOrder[minQuality];

    return references.filter(ref => qualityOrder[ref.quality] >= minLevel);
  }

  /**
   * Trie les r√©f√©rences par priorit√© (type + confiance)
   */
  sortByPriority(references: DetectedReference[]): DetectedReference[] {
    const typePriority: Record<string, number> = {
      door: 10,
      doorFrame: 9,
      tile: 8,
      window: 7,
      lightSwitch: 6,
      outlet: 5,
      baseboard: 4
    };

    return references.sort((a, b) => {
      const priorityDiff = (typePriority[b.type] || 0) - (typePriority[a.type] || 0);
      if (priorityDiff !== 0) return priorityDiff;
      
      // Si m√™me priorit√©, trier par confiance
      return b.confidence - a.confidence;
    });
  }
}

// Instance singleton
export const referenceObjectDetector = new ReferenceObjectDetector();

